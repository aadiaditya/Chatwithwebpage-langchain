# Chat with Webpage üåê

This Streamlit application allows you to chat with a webpage using a local Llama3 model and Retrieval-Augmented Generation (RAG) technique. By providing a webpage URL, the app extracts information from the page, creates embeddings, and allows you to ask questions about the content.

## Features

- **Webpage Data Loading**: Load content from any webpage URL.
- **Text Splitting**: Split the webpage content into manageable chunks for processing.
- **Embeddings and Vector Store**: Create embeddings using Ollama's Llama3 model and store them in a vector store.
- **Question Answering**: Ask questions about the webpage content and get responses generated by the Llama3 model.

## Setup

### Prerequisites

- Python 3.7 or later
- Streamlit
- Ollama
- LangChain

### Installation

1. **Clone the repository**:
    ```bash
    git clone https://github.com/your-repo/chat-with-webpage.git
    cd chat-with-webpage
    ```

2. **Create a virtual environment**:
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3. **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

### Running the App

1. **Start the Streamlit app**:
    ```bash
    streamlit run app.py
    ```

2. **Interact with the App**:
    - Open the app in your browser (usually at `http://localhost:8501`).
    - Enter the webpage URL in the provided input field and load the data.
    - Ask any question about the loaded webpage content and get the response generated by the Llama3 model.

## Code Overview

### app.py

This is the main file that sets up the Streamlit app and the RAG-based question-answering system.

- **Imports**: Necessary libraries and tools from Streamlit, Ollama, and LangChain.
- **Streamlit UI**: Defines the title, input fields, and chat interface.
- **Data Loading**: Uses `WebBaseLoader` to load and process webpage content.
- **Text Splitting**: Splits the webpage content into chunks using `RecursiveCharacterTextSplitter`.
- **Embeddings and Vector Store**: Creates embeddings with Ollama's Llama3 model and stores them in Chroma vector store.
- **Question Answering**: Defines functions to retrieve relevant documents and generate responses using the Llama3 model.

### Dependencies

- **Streamlit**: For the web interface.
- **Ollama**: For the Llama3 language model.
- **LangChain**: For text splitting, document loading, and vector store management.
